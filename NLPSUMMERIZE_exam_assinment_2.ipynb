{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtfA0lMXxB2qLs0ZeaPzPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armansheikhibiglari/PRACTICE-2/blob/main/NLPSUMMERIZE_exam_assinment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2-begr8EdN3",
        "outputId": "1d889ee7-18dd-4285-8a5e-e37d46f4438e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer as stemmer\n",
        "from nltk import FreqDist\n",
        "from nltk.classify import apply_features\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk.cluster.util import cosine_distance\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNSt4lHUFMg_",
        "outputId": "632e4368-650e-4dac-c7c8-fbc5a168fd14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "class HierarchicalSummarizer:\n",
        "    def __init__(self, context_window=4000):\n",
        "        self.context_window = context_window\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def normalize_sentence(self, sentence):\n",
        "        \"\"\"Tokenize and remove stop words.\"\"\"\n",
        "        return [word.lower() for word in word_tokenize(sentence) if word.isalnum() and word.lower() not in self.stop_words]\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        return dot_product / (norm_vec1 * norm_vec2) if norm_vec1 and norm_vec2 else 0\n",
        "\n",
        "    def sentence_vector(self, sentence, vocab):\n",
        "        \"\"\"Create a vector representation of a sentence based on a vocabulary.\"\"\"\n",
        "        vector = np.zeros(len(vocab))\n",
        "        for word in sentence:\n",
        "            if word in vocab:\n",
        "                vector[vocab.index(word)] += 1\n",
        "        return vector\n",
        "\n",
        "    def summarize(self, text, target_length):\n",
        "        \"\"\"Summarize the text to the target length.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        normalized_sentences = [self.normalize_sentence(sentence) for sentence in sentences]\n",
        "        vocab = list(set(word for sentence in normalized_sentences for word in sentence))\n",
        "        sentence_vectors = [self.sentence_vector(sentence, vocab) for sentence in normalized_sentences]\n",
        "\n",
        "        # Rank sentences by importance using cosine similarity\n",
        "        scores = [sum(self.cosine_similarity(vec, sentence_vectors[j]) for j in range(len(sentences)) if i != j)\n",
        "                  for i, vec in enumerate(sentence_vectors)]\n",
        "        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        # Return the top sentences based on the target length\n",
        "        return ' '.join(sentence for _, sentence in ranked_sentences[:target_length])\n",
        "\n",
        "    def hierarchical_summarize(self, text):\n",
        "        \"\"\"Perform hierarchical summarization for large texts.\"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        print(f\"Original document length (tokens): {len(tokens)}\")\n",
        "\n",
        "        # Check if the document is already within the context window\n",
        "        if len(tokens) <= self.context_window:\n",
        "            return self.summarize(text, target_length=int(len(tokens) * 0.1))  # Proportional summary for small texts\n",
        "\n",
        "        # Slice and summarize in chunks\n",
        "        slices = [tokens[i:i + self.context_window] for i in range(0, len(tokens), self.context_window)]\n",
        "        summarized_slices = [self.summarize(' '.join(slice), target_length=int(len(slice) * 0.1)) for slice in slices]\n",
        "\n",
        "        # Combine summaries and shrink if necessary\n",
        "        combined_summary = ' '.join(summarized_slices)\n",
        "        if len(word_tokenize(combined_summary)) > self.context_window:\n",
        "            return self.hierarchical_summarize(combined_summary)\n",
        "\n",
        "        return combined_summary\n",
        "\n",
        "    def save_summary(self, summary, filename=\"summary.txt\"):\n",
        "        \"\"\"Save the summary to a file.\"\"\"\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(summary)\n",
        "\n",
        "    def generate_query(self, summary):\n",
        "        \"\"\"Generate a query based on the summary.\"\"\"\n",
        "        # This can be refined depending on the query's desired form.\n",
        "        query = \"Query: \" + summary[:50] + \"...\"  # A simple example query based on the summary\n",
        "        return query\n",
        "\n",
        "\n",
        "# Example usage\n",
        "documents = [\n",
        "    \"The validation set is used during the training process to assess the performance of a machine learning model. \"\n",
        "    \"The purpose of the validation set is to provide an unbiased evaluation of the model's generalization ability. \"\n",
        "    \"It helps to tune the model parameters and prevent overfitting. In this context, it is a crucial element of model evaluation.\"\n",
        "]\n",
        "\n",
        "summarizer = HierarchicalSummarizer()\n",
        "\n",
        "for i, doc in enumerate(documents, start=1):\n",
        "    print(f\"\\nDocument {i} Summary:\")\n",
        "    summary = summarizer.hierarchical_summarize(doc)\n",
        "    print(summary)\n",
        "\n",
        "    # Save the summary to a file\n",
        "    summarizer.save_summary(summary, filename=f\"document_{i}_summary.txt\")\n",
        "\n",
        "    # Generate a query based on the summary\n",
        "    query = summarizer.generate_query(summary)\n",
        "    print(f\"Generated Query: {query}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NWPkmcRZ9Bi",
        "outputId": "8d63eac6-ddd1-4b2f-cd36-d569ced48ab6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 Summary:\n",
            "Original document length (tokens): 62\n",
            "The purpose of the validation set is to provide an unbiased evaluation of the model's generalization ability. In this context, it is a crucial element of model evaluation. The validation set is used during the training process to assess the performance of a machine learning model. It helps to tune the model parameters and prevent overfitting.\n",
            "Generated Query: Query: The purpose of the validation set is to provide an...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}