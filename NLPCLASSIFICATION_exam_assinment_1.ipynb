{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/h1x9fYnS38bjYybhclbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armansheikhibiglari/PRACTICE-2/blob/main/NLPCLASSIFICATION_exam_assinment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxnc41cR_Qke",
        "outputId": "cca32813-d74f-4f59-e4fd-9392b7b3d133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=090bfdd2f27d8dcae7726154489c2826f50aba09d543d34fffb27053c25ee699\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer as stemmer\n",
        "from nltk import FreqDist\n",
        "from nltk.classify import apply_features\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re"
      ],
      "metadata": {
        "id": "RnZE5FaB_oD6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIo_CMEoBLwt",
        "outputId": "55bf1a02-2c89-416c-abbe-13aff20d40a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "836OOL9pBeMW",
        "outputId": "7832e31e-6b91-4d94-e002-e1160c324be2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24T84d8rBmE1",
        "outputId": "4c6a8359-4403-4119-bf2e-b29fd7cc04d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyu9ZCD9Bn7R",
        "outputId": "d9535530-de51-41bb-88b5-96eac74e17d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_tokenize_sp_case(text):\n",
        "    # Tokenize the text\n",
        "    tokens_word = word_tokenize(text)\n",
        "\n",
        "    # Apply stemming to each token and join them back into a string\n",
        "    stem_text = ' '.join([stemmer.stem(word) for word in tokens_word])\n",
        "\n",
        "    return stem_text\n"
      ],
      "metadata": {
        "id": "TXLgKgn7CCaR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)  # Remove non-alphabetic characters\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Function to fetch data from Wikipedia API\n",
        "def fetch_wikipedia_data(categories, label, max_articles=100):\n",
        "    articles = []\n",
        "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    for category in categories:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"categorymembers\",\n",
        "            \"cmtitle\": f\"Category:{category}\",\n",
        "            \"cmlimit\": max_articles,\n",
        "            \"format\": \"json\",\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        for page in data.get(\"query\", {}).get(\"categorymembers\", []):\n",
        "            if \"title\" in page:\n",
        "                articles.append((page[\"title\"], label))\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Fetching geographic and non-geographic articles\n",
        "geographic_categories = [\"Geography\", \"Maps\"]\n",
        "non_geographic_categories = [\"History\", \"Engineering\", \"Art\"]\n",
        "\n",
        "data = fetch_wikipedia_data(geographic_categories, label=\"geographic\")\n",
        "data += fetch_wikipedia_data(non_geographic_categories, label=\"non-geographic\")\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
        "\n",
        "# Preprocess the text data\n",
        "df[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"cleaned_text\"]).toarray()\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Naive Bayes Classification Report:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "\n",
        "# Logistic Regression classifier\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFvKfsovXDBN",
        "outputId": "f65529c0-d9a3-4e05-c0cb-c696c7969e96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    geographic       0.83      0.95      0.88        40\n",
            "non-geographic       0.94      0.79      0.86        38\n",
            "\n",
            "      accuracy                           0.87        78\n",
            "     macro avg       0.88      0.87      0.87        78\n",
            "  weighted avg       0.88      0.87      0.87        78\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    geographic       0.81      0.97      0.89        40\n",
            "non-geographic       0.97      0.76      0.85        38\n",
            "\n",
            "      accuracy                           0.87        78\n",
            "     macro avg       0.89      0.87      0.87        78\n",
            "  weighted avg       0.89      0.87      0.87        78\n",
            "\n"
          ]
        }
      ]
    }
  ]
}